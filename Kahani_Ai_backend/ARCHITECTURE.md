# System Architecture

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACE                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Web UI (HTML)  â”‚              â”‚   REST API       â”‚         â”‚
â”‚  â”‚  - Story Writer  â”‚              â”‚   - cURL         â”‚         â”‚
â”‚  â”‚  - Lore Viewer   â”‚              â”‚   - Postman      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI APPLICATION                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    main.py (API Layer)                    â”‚  â”‚
â”‚  â”‚  â€¢ /api/story/suggest      â€¢ /api/lore/extract           â”‚  â”‚
â”‚  â”‚  â€¢ /api/story/edit         â€¢ /api/lore/all               â”‚  â”‚
â”‚  â”‚  â€¢ /api/story/verify       â€¢ /api/context/retrieve       â”‚  â”‚
â”‚  â”‚  â€¢ /api/story/canonicalize â€¢ /health                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚              â”‚              â”‚                    â”‚
â”‚              â–¼              â–¼              â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ RAG Service â”‚  â”‚  LLM Service â”‚  â”‚ Embedding    â”‚          â”‚
â”‚  â”‚             â”‚  â”‚              â”‚  â”‚ Service      â”‚          â”‚
â”‚  â”‚ â€¢ retrieve  â”‚  â”‚ â€¢ LLM-1      â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ â€¢ generate  â”‚  â”‚ â€¢ LLM-2      â”‚  â”‚ â€¢ generate   â”‚          â”‚
â”‚  â”‚             â”‚  â”‚ â€¢ extract    â”‚  â”‚ â€¢ batch      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚              â”‚              â”‚
              â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXTERNAL SERVICES                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Milvus DB   â”‚  â”‚  Groq Cloud  â”‚  â”‚  SQLite DB   â”‚          â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚          â”‚
â”‚  â”‚ â€¢ Vectors    â”‚  â”‚ â€¢ Llama 3.1  â”‚  â”‚ â€¢ StoryLine  â”‚          â”‚
â”‚  â”‚ â€¢ Search     â”‚  â”‚ â€¢ Context.   â”‚  â”‚ â€¢ LoreEntry  â”‚          â”‚
â”‚  â”‚ â€¢ IVF Index  â”‚  â”‚ â€¢ Creative   â”‚  â”‚ â€¢ Canonical  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BACKGROUND TASKS                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              APScheduler (background_tasks.py)            â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  â±ï¸  Every 30 min: Extract Lore                           â”‚  â”‚
â”‚  â”‚  â±ï¸  Every 60 min: Generate Summaries                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Flow - Story Creation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER    â”‚
â”‚  "A hero â”‚
â”‚  enters" â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 1. POST /api/story/suggest
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG Service    â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Embedding â”‚  â”‚ â† 2. Convert prompt to vector
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        â”‚        â”‚
â”‚        â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Milvus   â”‚  â”‚ â† 3. Search similar vectors
â”‚  â”‚  Search   â”‚  â”‚    (story_line, lore, summary)
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ 4. Retrieved context (top 5-10)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Service    â”‚
â”‚  (Groq Cloud)   â”‚
â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM-1    â”‚  â”‚ â† 5. Generate suggestion
â”‚  â”‚  Llama    â”‚  â”‚    with context
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ 6. Return suggestion + context
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER            â”‚
â”‚  Reviews & Edits â”‚
â”‚  Signs âœ…        â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ 7. POST /api/story/edit
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (Background Task)          â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  SQLite DB   â”‚ â† 8. Store line  â”‚
â”‚  â”‚  StoryLine   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Embedding   â”‚ â† 9. Generate    â”‚
â”‚  â”‚  Service     â”‚     embedding    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚        â”‚                            â”‚
â”‚        â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Milvus DB   â”‚ â† 10. Store      â”‚
â”‚  â”‚  Insert      â”‚      vector      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Background Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        APScheduler Triggers                 â”‚
â”‚                                             â”‚
â”‚  â±ï¸  Every 30 minutes                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Lore Extraction Task                â”‚
â”‚                                      â”‚
â”‚  1. Get recent story lines (20)     â”‚
â”‚     â†“                                â”‚
â”‚  2. Extract with LLM:                â”‚
â”‚     â€¢ Characters                     â”‚
â”‚     â€¢ Locations                      â”‚
â”‚     â€¢ Events                         â”‚
â”‚     â€¢ Items                          â”‚
â”‚     â†“                                â”‚
â”‚  3. Create embeddings                â”‚
â”‚     â†“                                â”‚
â”‚  4. Store in:                        â”‚
â”‚     â€¢ SQLite (LoreEntry)             â”‚
â”‚     â€¢ Milvus (vectors)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Summary Generation Task             â”‚
â”‚                                      â”‚
â”‚  1. Get all verified lines           â”‚
â”‚     â†“                                â”‚
â”‚  2. Chunk into groups (10 lines)    â”‚
â”‚     â†“                                â”‚
â”‚  3. Generate summary per chunk       â”‚
â”‚     â†“                                â”‚
â”‚  4. Create embedding                 â”‚
â”‚     â†“                                â”‚
â”‚  5. Store in Milvus                  â”‚
â”‚     (content_type: "summary")        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ LLM Roles

### LLM-1: Story Suggester

```
Input:
  â€¢ User prompt
  â€¢ Retrieved context from RAG

Process:
  â€¢ Model: llama-3.1-70b-versatile
  â€¢ Temperature: 0.7 (creative)
  â€¢ Max tokens: 200

Output:
  â€¢ Short story line(s) (1-3 sentences)
  â€¢ Contextually relevant
  â€¢ Natural continuation
```

### LLM-2: Canonicalizer

```
Input:
  â€¢ All verified story lines

Process:
  â€¢ Model: llama-3.1-70b-versatile
  â€¢ Temperature: 0.5 (balanced)
  â€¢ Max tokens: 2000

Output:
  â€¢ Polished narrative
  â€¢ Proper paragraphs
  â€¢ Fixed inconsistencies
  â€¢ Professional quality
```

### LLM Extractor

```
Input:
  â€¢ Story lines to analyze

Process:
  â€¢ Model: llama-3.1-70b-versatile
  â€¢ Temperature: 0.3 (precise)
  â€¢ Output format: JSON

Output:
  â€¢ Structured lore data
  â€¢ Characters, locations, events, items
  â€¢ With descriptions
```

---

## ğŸ—„ï¸ Database Schema

### SQLite Tables

```sql
-- Story Lines
CREATE TABLE story_lines (
    id INTEGER PRIMARY KEY,
    user_id VARCHAR,
    line_text TEXT NOT NULL,
    line_number INTEGER,
    context_used TEXT,
    llm_proposed TEXT,
    user_edited BOOLEAN,
    verified BOOLEAN,
    signature VARCHAR,
    embedding_id VARCHAR,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Lore Entries
CREATE TABLE lore_entries (
    id INTEGER PRIMARY KEY,
    entity_type VARCHAR,  -- character, location, event, item
    entity_name VARCHAR,
    description TEXT,
    source_lines TEXT,
    confidence FLOAT,
    embedding_id VARCHAR,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- Canonical Stories
CREATE TABLE canonical_stories (
    id INTEGER PRIMARY KEY,
    title VARCHAR,
    full_text TEXT NOT NULL,
    canonicalized_by VARCHAR,
    original_lines_count INTEGER,
    version INTEGER,
    created_at TIMESTAMP,
    finalized_at TIMESTAMP
);
```

### Milvus Collection Schema

```python
Collection: "story_embeddings"

Fields:
  - id: INT64 (primary, auto_id)
  - embedding_id: VARCHAR(100)
  - text: VARCHAR(5000)
  - content_type: VARCHAR(50)  # story_line, lore_character, summary
  - embedding: FLOAT_VECTOR(384)  # all-MiniLM-L6-v2

Index:
  - Type: IVF_FLAT
  - Metric: L2 distance
  - nlist: 128
  - nprobe: 10
```

---

## ğŸ” Security & Verification

```
User signs story line
        â†“
Backend creates signature:
  SHA256(line_text)[:16]
        â†“
Stores in database:
  - verified = True
  - signature = hash
        â†“
Future verification:
  - Compare stored hash
  - Ensure integrity
```

---

## ğŸ“ˆ Scalability Considerations

### Current Setup (Single Instance)

- âœ… Perfect for personal use
- âœ… 1-10 concurrent users
- âœ… Thousands of story lines

### Future Scaling Options

- ğŸ”„ PostgreSQL instead of SQLite
- ğŸ”„ Milvus cluster (distributed)
- ğŸ”„ Redis for caching
- ğŸ”„ Celery for background tasks
- ğŸ”„ Load balancer for API
- ğŸ”„ Vector DB sharding

---

## ğŸ¯ Performance Optimization

### Vector Search

- **Index Type**: IVF_FLAT (good for <1M vectors)
- **For larger**: Switch to IVF_PQ or HNSW
- **nprobe**: Balance speed vs accuracy

### Embedding Generation

- **Batch processing**: Process multiple texts at once
- **Caching**: Cache embeddings for repeated queries
- **Model**: all-MiniLM-L6-v2 (fast, 384 dim)

### LLM Calls

- **Groq**: Ultra-fast inference (~500 tokens/sec)
- **Context limits**: Keep under 4096 tokens
- **Batching**: Queue multiple requests

---

**Architecture Complete! ğŸ—ï¸âœ¨**
